 야구를 좋아해서 데이터를 활용하는 쪽으로 공부의 방향성과 진로를 잡았습니다. 야구에 대한 이해를 높이고자 과학고에 진학했습니다. 과학고에서 수학과 과학을 통해 현상을 설명하고 원리를 파악하는 일이 재미있었습니다. 대학에서는 경제학을 선택했습니다. 경제학을 배우면서 단순히 수치상으로 효율이 좋고 합리적인 선택을 넘어 개인마다 다른 가치가 존재하기에 이를 바탕으로 실제 시장에서 거래가 일어나고 사회가 돌아가는 과정을 이해했습니다. 머니볼처럼 새로운 가치를 데이터로 찾아내는 것이 제가 하고 싶은 일이었습니다.

 25년 2월부터 포스코에서 ai 빅데이터 아카데미를 수강했습니다. 이때 두가지 프로젝트를 진행했습니다. 첫번째는 데이터분석 프로젝트였습니다. 저희 조는 제조업에 관심있는 팀원이 많아 주제를 철강으로 선정하였습니다. 이런 철강의 도메인은 저는 완전히 모르는 영역이었습니다. 그렇기에 철강 공정의 원리부터 학습했습니다. 연주 공정의 물리적 원리를 파악하고 논문을 통해 불량이 발생하는 이론적 근거를 탐색했습니다. 이를 바탕으로 데이터 분석 플로우를 설계해서 팀에 방향성을 제시했습니다. 이 과정에서 단순히 데이터 분석만 하는 것이 아닌 실질적으로 도출가능한 개선안을 내야한다고 문제를 정의하고 이를 구조화하며 변경 리스크를 고려하는 실질적인 기업의 의사결정 과정을 경험했습니다.

 두번째는 ai기술을 활용한 자율 프로젝트였습니다. 이때 주제선정에서 많은 시간이 걸렸습니다. 저는 아이디어보다 실현가능성에 높은 가중치를 두었습니다. 당시에 저희는 ai기술에 대해 잘 몰랐고 ai와 함께하려 해도 환각현상이 높았기에 주어진 시간에 완수할 수 있어야 한다고 여겼습니다. 팀원들에게 말로 설득하기보다 WSL 환경에서 프로토타입을 직접 구현해서 작동하는 결과물을 먼저 보여주었습니다. 이론적으로 가능하다는 것이 아니라 실제로 돌아간다는 사실이 팀을 설득하는 가장 빠른 방법이었습니다.

 25년 여름 이후 ai가 빠르게 발전함을 확인하고 경험했습니다. 이때부터 단순 질문과 답변이 아닌 프로젝트를 함께 실행하는 역할로 ai를 사용했습니다. 처음 시도한 것은 저의 도메인 지식과 관심분야에 대한 전문성을 ai와 함께 높여가는 것이었습니다. 논문의 형식으로 거시환경을 분석하는 것을 진행했습니다. 이 논문은 제가 데이터 분석에 임하는 태도를 가장 잘 보여주는 작업입니다.

 2024년 하반기는 연준의 통화정책 전환기였습니다. 시장은 빠르게 움직였습니다. 왜 그렇게 움직였는지에 대한 설명은 사후적이고 정성적인 수준에 머물러 있었습니다. 약 50개의 거시경제 변수를 활용하여 시장의 정책금리 기대 변화를 체계적으로 설명하는 구조를 만들고자 했습니다. 목적은 예측이 아니라 설명력이었습니다. 어떤 변수가 시장의 정책 기대를 실질적으로 움직이는지 통계적으로 식별하는 것이 핵심 질문이었습니다.

 50개 변수를 모두 투입하면 과적합이 발생하고 임의로 줄이면 정보 손실이 나타납니다. 이 문제를 해결하기 위해 3단계 추정 프레임워크를 설계했습니다. LASSO로 고차원 변수 중 설명력 있는 변수만 선택했습니다. Post-LASSO OLS로 LASSO가 선택한 변수만으로 회귀를 재추정하여 축소 편향을 제거했습니다. Newey-West HAC 표준오차를 적용하여 금융 시계열 데이터의 자기상관과 이분산을 보정했습니다. 각 단계가 이전 단계의 한계를 보완하는 구조입니다.

 초기 모델 분석 결과 Baa 등급 회사채 스프레드가 시장 기대를 설명하는 핵심 변수처럼 보였습니다. 보통의 경우라면 여기서 결론을 내렸겠지만 저는 시계열 데이터 특성상 특정 시점에만 데이터가 과적합되었을 가능성을 의심했습니다. 100거래일 윈도우를 20거래일씩 이동시키며 동일한 추정을 68회 반복하는 롤링 분석을 수행했습니다. 결과는 예상과 달랐습니다. Baa 스프레드는 선택 빈도가 불안정했습니다. 계수 부호도 시점에 따라 뒤집혔습니다. 반면 투자등급 회사채 수익률인 Ret_Corp_InvGrade는 68회 추정 중 73%의 선택 빈도와 100%의 부호 일관성을 보였습니다. 가장 강건한 변수였습니다.

 초기 결과에서 멈췄다면 Baa 스프레드가 핵심 변수라는 틀린 결론을 내렸을 것입니다. 68회 반복 검증을 통해 구조적 안정성이 검증된 결론에 도달할 수 있었습니다. 한 번의 결과를 맹신하지 않고 그 결과가 얼마나 안정적인지 집요하게 검증하는 것이 이 논문을 통해 세운 제 분석 태도입니다.

 2024년 12월 연준의 Dot Plot 발표 전후로 시장의 반응 구조가 달라졌는지를 Chow Test로 검증했습니다. R-squared가 0.67에서 0.76으로 상승했습니다. 달러 인덱스와 신용 스프레드의 회귀 계수가 구조적으로 변화했습니다. 변수의 값이 바뀐 것이 아니라 시장이 정보를 처리하는 방식 자체가 변한 것입니다. 정책 정보가 시장의 반응함수를 바꿨다는 것을 통계적으로 입증했습니다. 이 발견이 논문의 핵심 기여입니다.

 논문이 데이터로 시장을 설명하는 구조를 만든 작업이었다면 이제 실제 돌아가는 프로그램을 구현하고 싶었습니다. 주어진 시간대에 대한 분석이 아닌 실시간으로 데이터를 수집하고 확인하면서 시장을 분석하고 다음 의사결정에 도움이 되는 근거를 마련하고자 했습니다. EIMAS를 설계하고 구현했습니다. 데이터 수집부터 분석, ai요약과 시장 확인 등의 내용부터 시작해서 금융 지식과 경제 도메인을 추가하는 식으로 확장시켰습니다.

 처음에는 api로 실시간 데이터를 수집하고 ai가 요약하는 단순한 구조로 시작했습니다. 금과 은과 희토류 거래량 이상을 탐지하고 섹터별 뉴스를 ai가 종합하는 수준이었습니다. 이는 단순히 특화된 llm api를 사용하고 ai가 요약하는 것에 불과했습니다. 설명할 수 없는 결과였습니다. 경제학적 기반으로 설명과 예측 과정을 이해할 수 있게 방향을 바꿨습니다.

 Python과 asyncio로 비동기 데이터 수집 구조를 만들었습니다. FRED api와 yfinance로 거시경제 지표와 시장 데이터를 수집했습니다. 분석에는 sklearn의 LASSO와 statsmodels의 회귀 분석을 사용했습니다. Claude와 GPT-4와 Gemini를 각 에이전트에 역할별로 배분했습니다.

 기관 투자자의 리서치 보고서는 대부분 정성적 서술입니다. 이 정성적 판단 속에 있는 정량적 논리를 추출하여 알고리즘으로 변환했습니다. JP모건의 버블 5단계 프레임워크를 레버리지 비율과 밸류에이션 배수로 점수화하는 알고리즘으로 구현했습니다. 골드만삭스의 갭 분석을 시장에 내재된 기대 성장률과 모델 예측 성장률 간의 괴리를 포착하는 로직으로 설계했습니다. Hamilton의 GMM으로 레짐 탐지를 구현했고 Bekaert의 VIX 분해와 Lopez de Prado의 HRP 포트폴리오 최적화도 코드로 만들어 실제 데이터에 적용했습니다.

 백테스팅에서 4년간 누적 수익률 8360%가 나왔습니다. 통계적으로 불가능한 수치였습니다. 의심했습니다. 전략 로직을 단위별로 분해하여 디버깅했고 숏 포지션의 거래 수수료가 누적 계산에서 누락되는 버그를 발견했습니다. 좋아 보이는 결과일수록 더 의심해야 한다는 것을 배웠습니다. 이후 모든 백테스팅 파이프라인에 데이터 무결성 체크 로직을 선행 단계로 포함시키는 것을 기본 원칙으로 삼았습니다.

 EIMAS의 핵심은 7개의 AI 에이전트가 서로 다른 관점에서 토론하는 구조입니다. 경제학자와 리스크 관리자와 퀀트 분석가 등 각기 다른 페르소나를 가진 에이전트가 동일한 시장 상황을 독립적으로 분석합니다. 상호 비판을 통해 편향을 제거합니다. 블랙박스 예측이 아닙니다. 각 에이전트가 왜 그렇게 판단했는지를 명시적으로 출력합니다. 인간 전문가 집단의 합의 과정을 Agentic AI로 자동화한 것입니다.

 이 구조는 실제로 작동했습니다. 2026년 1월 말 기준 실제 시장 데이터를 투입한 분석에서 시스템은 Fed Funds 내재금리 3.64%와 Net Liquidity 5조 7천억 달러 수준의 풍부한 유동성 그리고 Bull Low Vol 레짐을 진단했습니다. FULL 모드와 REFERENCE 모드 모두 BULLISH 판단으로 일치했습니다. 신뢰도는 65에서 75% 사이였습니다. 리스크 점수는 낮았지만 왜 이런 점수가 나오는지 더 의심하고 확인해보려 했습니다. 경제학자 페르소나 에이전트가 역사적 저점의 리스크 점수는 오히려 시장 과신의 신호일 수 있다고 경고했습니다. 이 경고는 최종 보고서에 리스크 요인으로 명시되었습니다. 결과를 출력하는 것이 아니라 에이전트 간 토론으로 편향을 제거하고 설명 가능한 근거를 도출하는 구조가 실제로 작동한 사례입니다.

 EIMAS는 Python 분석 시스템에 그치지 않았습니다. Next.js와 FastAPI를 결합한 웹 대시보드를 직접 개발했습니다. 실시간 레짐 현황과 리스크 점수와 포트폴리오 배분 현황을 시각화했습니다. 분석 결과가 의사결정자에게 전달되지 못하면 의미가 없다고 생각했습니다. 비전문가도 현재 시장 레짐이 무엇인지 리스크 수준이 어느 정도인지를 직관적으로 파악할 수 있도록 설계했습니다. 데이터 수집과 분석에서 시스템 구현과 사용자 전달까지 end-to-end로 직접 구현했습니다.

 기능을 추가하면서 시스템이 빠르게 비대해졌습니다. 단일 파일이 1000줄을 넘기 시작했습니다. 기능별로 패키지를 분리하고 기존 코드와의 호환성을 유지하는 shim 구조로 리팩토링했습니다. 에이전트를 구현하는 것에서 나아가 신뢰성 있게 작동하도록 하는 인프라 설계가 다음 과제였습니다.

 MCP 기반의 7단계 에이전트 파이프라인을 만들었습니다. data-validator에서 시작하여 macro-analyst와 signal-interpreter와 risk-mgr을 거쳐 quant-coder와 report-writer로 이어지는 구조입니다. 각 단계는 이전 단계의 출력을 검증한 후 다음 단계로 전달합니다. Goldilocks와 Overheating과 Stagflation과 Recession 네 가지 레짐 판단을 자동화합니다. 에이전트를 작동시키는 것이 아니라 신뢰성 있게 운영되도록 하는 하니스를 설계하는 것이 핵심이었습니다. 에이전트가 할루시네이션을 일으키거나 잘못된 데이터를 전달했을 때 파이프라인 전체가 무너지지 않도록 각 단계에 검증 로직과 실패 처리 메커니즘을 넣었습니다.

 AI 에이전트가 올바르게 판단하려면 도메인 지식이 정확하고 완전해야 합니다. 이를 관리하기 위해 3단계 감사 프로세스를 설계했습니다. Source Inventory 단계에서 학술 논문과 기관 보고서와 공식 문서 등 원천 자료를 목록화했습니다. Diff Mapping 단계에서 원천 자료의 지식과 현재 에이전트 스킬 파일에 반영된 지식 간의 차이를 매핑했습니다. Gap Classification 단계에서 누락된 공식과 수식과 구현 패턴을 분류하고 우선순위를 매겨 에이전트 스킬에 내재화했습니다. 단순히 AI를 사용하는 것이 아니라 AI가 더 잘 판단하도록 도메인 지식 자체를 설계하는 작업이었습니다. 에이전트의 성능은 모델의 크기가 아니라 주입되는 도메인 지식의 품질에 의해 결정된다는 것을 실무에서 확인했습니다. 지금도 계속 발전시키고 있습니다.

 포스코 아카데미에서 모르는 도메인의 비즈니스 문제를 데이터 구조로 변환하는 법을 배웠습니다. 논문을 통해 한 번의 결과에 안주하지 않고 구조적 안정성을 검증하는 태도를 세웠습니다. EIMAS에서는 기관의 정성적 판단을 알고리즘으로 구현하고 검증 원칙을 실전에 적용했습니다. 최근에는 에이전트가 신뢰성 있게 작동하도록 하는 하니스 설계와 도메인 지식 내재화까지 영역을 넓혔습니다.

 EIMAS에서 분석 결과를 의사결정자에게 전달하는 구조를 만들었습니다. 그 결정이 실제로 어떤 결과를 가져왔는지 수치로 검증하는 루프는 닫혀 있지 않았습니다. 에이브랩스의 Decision Science 접근이 그 루프를 닫는 방식입니다. 금융과 패션과 리테일 등 다양한 도메인에서 데이터로 정의한 문제를 에이전트가 실행하고 결과를 검증하는 구조를 AACE와 스마트 리포트에 실제로 적용하는 일을 에이브랩스에서 하고 싶습니다.

포트폴리오 / 코드: [GitHub 링크]
