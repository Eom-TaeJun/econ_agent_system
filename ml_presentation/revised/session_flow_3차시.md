# 3차시 강의 대본 — 나무로 규칙을 만든다

> 시간: 60분 | 대상: 베이스 혼합(초급~중급) | 톤: 직관에서 원리로, 그 다음 실무로 | 슬라이드: 18장

---

## [연결] 2차시 → 3차시 브리지 (0~3분)

### → 슬라이드 3-01: 직선이 못 잡는 패턴들

```
"지난 시간에 뭘 배웠죠?

 과적합에 대해서.

 '훈련 데이터는 90% 정확도, 실제 데이터는 50% 정확도'
 이 문제를 정규화로 어떻게 해결했어요.

 Ridge, Lasso... 계수를 제약해서 분산을 줄였죠.

 근데 여기서 한 가지 더 깊은 문제가 있어요.

 지난 시간 모든 예제가 뭐였나 생각해보세요.

 회귀 직선이었어요.

 X축 (변수) → Y축 (결과)가 '일직선'이라고 가정했어요.

 그런데 정말로 세상이 직선으로 이루어져 있을까요?

 예를 들어:

 '소득이 오를수록 행복도 계속 오르나요?'

 우리 경험상 알아요. 아니잖아요.

 처음엔 소득이 올라가면서 행복도 올라가지만,
 어느 순간부터는 올라가지 않아요.

 심지어 떨어지기도 해요.

 이게 비선형(non-linear) 관계거든요.

 곡선이에요.

 U자형, 역 N자형, S자형... 수십 가지 패턴이 있어요.

 '직선으로 곡선을 표현할 수 있을까?'

 답: 못 한다.

 그럼 뭘 하나?

 곡선 자체를 그린다.

 그런데 어떻게?

 오늘의 질문이 바로 이거예요."
```

**[강조]**
```
"오늘 배울 게 정확히 이겁니다:

 '데이터를 직선으로 못 설명하면,
  규칙으로 나눠서 설명할 수 있다'

 이 아이디어가 의사결정나무(Decision Tree)입니다."
```

→ **실무에서는**: 금융에서 "직선 모델로는 신용 위험을 못 봤지만, 의사결정나무를 쓰니까 숨겨진 위험군이 보였다"는 사례가 많아요.

**[예상 질문]** "비선형을 표현하는 방법이 여러 개 있지 않나요?"
→ "맞아요. 다항 회귀, 스플라인, 신경망 등이 있어요. 근데 오늘은 '가장 직관적이고 해석 가능한' 나무를 배웁니다."

---

## [개념] 의사결정나무 (3~21분)

### → 슬라이드 3-02: 스무고개로 분류한다

```
"스무고개라는 게임 아시죠?

 한 사람이 대상을 정하고,
 나머지가 예/아니오 질문을 20개까지 해서
 그 대상을 맞히는 놀이.

 '살아있나요?'
 '한국 사람인가요?'
 '현재 정치인인가요?'
 ... 이렇게.

 의사결정나무가 정확히 이겁니다.

 '변수 A가 5보다 큰가?'
 '그렇다면 변수 B가 10을 넘나?'
 '그렇다면 변수 C는 낮은가?'
 ... 이렇게 반복해서

 데이터를 그룹으로 나누는 거예요.

 그 그룹의 특징을 이용해서
 최종 판단을 내리는 것.

 [구조 이미지 상상]

                     (전체 1,000명)
                           │
                  변수A > 5인가?
                     /           \\
                   YES            NO
                  /                 \\
           (750명)                (250명)
              │                       │
          변수B > 10인가?        변수B > 10인가?
           /     \\                 /     \\
         YES     NO              YES      NO
        /         \\              /        \\
     (500명)  (250명)        (150명)  (100명)
        │        │              │        │
      리프1    리프2           리프3    리프4
   (이탈:90%) (이탈:40%) (이탈:70%) (이탈:20%)

 보세요. 각 끝 부분(리프)에 가면
 데이터가 한 종류에 가까워져요.

 리프 1은 이탈 고객 90%.
 리프 4는 이탈 고객 20%.

 이게 의사결정나무의 원리입니다.

 '질문을 반복해서 순수한 그룹을 만든다'는 거죠."
```

→ **실무에서는**: 대출 시스템이 "이 고객은 리프 3에 속하니까, 이탈 위험이 높은 고객 그룹"이라고 판단해서 금리를 높여요.

**[예상 질문]** "질문을 무한정 할 수 있나요?"
→ "이론상 맞지만, 실무에선 과적합을 막기 위해 최대 깊이를 제한합니다. 보통 3~7단계정도로."

---

### → 슬라이드 3-03: IF-THEN 규칙 구조

```
"이 구조를 규칙으로 쓰면 이렇게 됩니다:

 IF (변수A > 5) AND (변수B > 10) THEN 이탈 위험 90%
 IF (변수A > 5) AND (변수B ≤ 10) THEN 이탈 위험 40%
 IF (변수A ≤ 5) AND (변수B > 10) THEN 이탈 위험 70%
 IF (변수A ≤ 5) AND (변수B ≤ 10) THEN 이탈 위험 20%

 이 규칙들의 중요한 특징이 뭔가 하면,

 '사람이 읽을 수 있다'는 거예요.

 선형 회귀의 수식:
 이탈 = 0.3 + 0.5*A + 0.02*B - 0.001*A*B + ε

 이건 뭐가 뭔지 직관적으로 이해가 안 돼요.

 하지만 나무의 규칙:
 'A가 크고 B도 크면 이탈 위험 90%'

 이건 누구나 이해해요.

 심지어 비즈니스 담당자도, 규제 담당자도.

 그래서 금융권에서 의사결정나무를 좋아합니다.

 '왜 이 고객을 거절했는가'를 명확하게 설명할 수 있거든요."
```

→ **실무에서는**: 금융감독위원회가 "AI 대출 거절의 근거를 설명하시오"라고 하면, 나무의 규칙을 그대로 제시하면 됩니다.

---

### → 슬라이드 3-04: 어떻게 나눌 것인가

```
"그런데 여기서 중요한 질문이 생겨요.

 '어떤 변수로 먼저 나눌까?'
 '어느 값에서 자를까?'

 예를 들어:

 소비자 이탈 데이터 1,000명 중
 - 이탈: 400명
 - 유지: 600명

 변수 A로 자르면?
 A > 5: 이탈 700명, 유지 300명 (비율 70:30)
 A ≤ 5: 이탈 -300명은 불가능... 아, 계산 실수.
 다시:
 A > 5: 이탈 350명, 유지 150명
 A ≤ 5: 이탈 50명, 유지 450명

 변수 B로 자르면?
 B > 10: 이탈 200명, 유지 200명 (비율 50:50)
 B ≤ 10: 이탈 200명, 유지 400명

 어느 게 더 좋은 분할일까?

 변수 A로 자른 결과를 봐요.

 오른쪽 그룹은 '이탈 350, 유지 150'
 왼쪽 그룹은 '이탈 50, 유지 450'

 오른쪽은 이탈이 대부분이고,
 왼쪽은 유지가 대부분이에요.

 즉, '순수하게(homogeneous)' 나뉘었어요.

 반면 변수 B로 자르면,
 오른쪽 그룹이 50:50으로 섞여있어요.

 '섞여있다'는 게 나무에는 최악입니다.

 왜냐면, 그 그룹에 들어간 사람이
 이탈할지 유지할지 예측할 수 없기 때문이에요.

 그래서 나무는 '가장 순수하게 나뉘는 분할'을 찾아요.

 이걸 측정하는 지표가 Gini 불순도예요.

 [직관]

 Gini 불순도 = '이 그룹이 얼마나 섞여있나'를 측정

 예시:
 - 100명 전부 이탈 → Gini = 0 (완벽! 순수함)
 - 50명 이탈, 50명 유지 → Gini = 0.5 (최악! 완전히 섞임)
 - 75명 이탈, 25명 유지 → Gini = 0.375 (그 중간)

 Gini가 작을수록 좋습니다.

 나무가 하는 일:
 '모든 변수, 모든 분할 지점에 대해
  Gini를 계산해서
  가장 Gini를 많이 줄이는 분할을 선택한다'

 이 과정을 반복합니다.

 처음: Gini = 0.48 (전체)
 첫 분할 후: Gini = 0.30 (감소!)
 두 번째 분할 후: Gini = 0.15
 ...

 이렇게 계속 나무를 키워가는 거예요."
```

**[강조]**
```
"Gini는 수식이 있지만, 직관만 기억하세요:

 '섞여있으면 Gini 높음'
 '순수하면 Gini 낮음'

 이것만으로도 충분해요."
```

→ **실무에서는**: 의사결정나무를 훈련할 때 sklearn은 자동으로 모든 분할을 시도해서 최적의 나무를 만듭니다. 우리가 할 건 그 결과를 읽고 해석하는 것.

---

### → 슬라이드 3-05: 의사결정나무의 강점

```
"의사결정나무가 왜 이렇게 인기 있을까요?

 세 가지 강점이 있어요.

 [강점 1] 해석 가능하다 (Interpretable)

 '이 고객이 이 리프에 속하는 이유가 뭔가요?'
 → 경로를 따라가면 된다.

 '변수 A > 5이고, 변수 B ≤ 10이니까'
 → 누구나 이해할 수 있다.

 반대로 신경망:
 '이 고객이 이 출력값을 가진 이유가 뭔가요?'
 → '모릅니다. 수백만 개의 가중치가 섞여있어서요.'
 → 이게 블랙박스죠.

 [강점 2] 전처리가 필요 없다

 회귀 모델:
 - 범주형 변수? → 인코딩 필요
 - 스케일링? → 필수 (정규화)
 - 이상치? → 제거 고민

 의사결정나무:
 - 범주형 변수? → 그냥 써도 된다
 - 스케일링? → 필요 없다 (크기 비교만 하니까)
 - 이상치? → 괜찮다 (한 리프에 들어갈 뿐)

 [강점 3] 직관적인 규칙이 나온다

 '이탈 고객의 특징이 뭔가요?'
 → 규칙을 읽으면 바로 나온다.

 '우리가 개입해야 할 고객이 누구인가요?'
 → 리프 1,3 같이 위험도가 높은 그룹.
 → 정책을 세울 수 있다.

 이런 강점들이 있어서,
 의사결정나무는 '설명이 중요한' 분야에서 표준입니다.

 금융, 의료, 법무..."
```

→ **실무에서는**: 은행의 대출심사팀이 의사결정나무를 선호하는 이유가 정확히 이것. "거절 이유를 설명할 수 있으니까."

---

### → 슬라이드 3-06: 의사결정나무의 약점

```
"좋은 점만 있으면 뭐합니까.

 큰 약점이 있어요.

 이를 이해하는 게 오늘 나머지 시간의 핵심입니다.

 [약점] 불안정하다 (Unstable)

 예를 들어:

 여러분이 고객 1,000명의 데이터로
 의사결정나무를 만들었어요.

 그 나무:
                     (전체 1,000명)
                           │
                  변수A > 5인가?
                     /           \\
                   YES            NO
                  /                \\
           (750명)                (250명)
                │                      │

 이제 고객이 1명 추가됩니다.

 새 나무:
                     (전체 1,001명)
                           │
                  변수C > 3인가?  ← 아, 변수 A가 아니라 C로 자르네?
                     /           \\
                   YES            NO
                  /                \\
           (501명)                (500명)
                │                      │

 분할 기준이 바뀌었어요!

 '고작 1명이 들어갔는데 나무가 완전히 달라졌다?'

 네. 이게 의사결정나무의 가장 큰 약점입니다.

 왜 이럴까?

 왜냐면 나무는 '그리디(greedy) 알고리즘'이거든요.

 '지금 가장 좋은 분할을 택한다'

 지금이 바뀌면, 최적의 분할도 바뀐다는 거죠.

 데이터 10개가 바뀌면?
 나무 전체 구조가 뒤바뀔 수도 있어요.

 [시각 상상]

 나무A:                      나무B:
    │                          │
    A                          C
   / \\                        / \\
  B   D                      B   E
 / \\  / \\                  / \\  / \\
G  H I  J                K  L M  N

 '똑같은 데이터인데 나무가 다르다?'

 네. 데이터의 사소한 변화에도
 나무가 크게 흔들린다는 뜻입니다.

 이 문제를 통계학으로는 'High Variance'라고 부릅니다.

 '높은 분산' = '데이터에 민감하게 반응한다' = '불안정하다'

 이게 우리가 지금부터 해결해야 할 문제입니다."
```

**[강조]**
```
"한 나무는 강력하지만 불안정하다.

 그럼 어떻게 안정화할까?

 다음 섹션에서 배울 건데,
 핵심 아이디어가 이거예요:

 '한 나무가 약하면, 많은 나무를 합치면 어떨까?'"
```

→ **실무에서는**: 모델을 배포했는데 2개월 뒤 성능이 70%에서 40%로 떨어지는 현상. 이건 보통 의사결정나무의 불안정성 때문입니다.

**[예상 질문]** "그럼 큰 데이터셋을 쓰면 안정적이 되나요?"
→ "어느 정도 도움이 되지만, 근본 문제는 못 고쳐요. 구조적 약점이기 때문이죠."

---

## [핵심] Bias-Variance Tradeoff (21~33분)

### → 슬라이드 3-07: Bias-Variance Tradeoff 2×2

```
"이제 중요한 개념을 배웁니다.

 이 개념이 모든 모델 선택의 기준이 돼요.

 '모델을 어떻게 평가할까?'
 '어떤 모델이 더 좋은가?'
 '왜 한 모델은 정확하고 다른 모델은 빗나갈까?'

 이 모든 질문에 답할 수 있게 해주는 틀입니다.

 그 틀의 이름이 Bias-Variance Tradeoff예요.

 [직관 — 과녁 비유]

 과녁을 맞히는 게임을 상상해봐요.

 여러 명이 같은 과녁에 화살을 10번씩 쏩니다.

 [모델 A]
      ●                    ← 정확하지만 분산 있음
        ●                     (뭉쳐있지 않음)
          ●
        ●
          ●

 과녁 중심에서 평균 0.1cm 거리 (정확!)
 하지만 화살들이 흩어져 있음 (불안정)
 → 낮은 Bias, 높은 Variance

 [모델 B]
   ●●●●●                ← 부정확하지만 안정적
   ●●●●●                   (완벽하게 뭉쳐있음)
   ●●●●●

 과녁 중심에서 평균 5cm 떨어짐 (부정확)
 하지만 화살들이 뭉쳐있음 (안정적)
 → 높은 Bias, 낮은 Variance

 실제 문제에 당면했을 때:

 [모델 A: 정확하지만 분산 있음]
 - 훈련 데이터: 95% 정확도
 - 테스트 데이터: 50% 정확도

 이유: 훈련 데이터에 완벽하게 맞춰졌지만,
       새 데이터에는 틀린다.
       = 과적합 = 높은 분산

 [모델 B: 부정확하지만 분산 낮음]
 - 훈련 데이터: 70% 정확도
 - 테스트 데이터: 68% 정확도

 이유: 훈련 데이터도 못 맞추고,
       테스트 데이터도 못 맞춘다.
       = 과소적합 = 높은 편향

 둘 다 안 좋아요.

 최고의 모델:
 - 훈련 데이터: 85% 정확도
 - 테스트 데이터: 83% 정확도

 둘 다 높고, 차이가 적다.
 = 적절한 Bias + 적절한 Variance 균형

 이게 바로 트레이드오프 개념입니다.

 [2×2 다이어그램]

              낮은 Bias    높은 Bias
 낮은 Variance   [최적]        [과소적합]
 높은 Variance   [과적합]      [최악]

 [최적] 구역:
 훈련 오차 ~낮음, 테스트 오차 ~낮음 → 좋다

 [과적합] 구역:
 훈련 오차 ~매우 낮음, 테스트 오차 높음 → 나무가 여기 있음

 [과소적합] 구역:
 훈련 오차도 높음, 테스트 오차도 높음 → 나무가 너무 작으면 여기

 [최악] 구역:
 높은 Bias + 높은 Variance → 나쁜 모델

 우리의 목표:
 '모든 모델을 [최적] 구역으로 옮기는 것'

 이 목표를 향해 3~4차시 전체가 움직입니다."
```

→ **실무에서는**: 모델 성능이 안 좋을 때 "Bias 문제인가 Variance 문제인가"를 먼저 진단하고, 그에 맞는 해결책을 씁니다.

**[예상 질문]** "과녁 비유가 이상한데, 실제론 뭐가 다른 건가요?"
→ "과녁에서 '화살 위치 변화'가 Variance이고, '과녁 중심과의 거리'가 Bias예요. 모델은 데이터 변화에 얼마나 반응하느냐 vs 실제 패턴을 얼마나 놓치는가가 달라요."

---

### → 슬라이드 3-08: 고편향 vs 고분산 모델

```
"Bias와 Variance를 더 구체적으로 봅시다.

 [고편향 모델 (High Bias)]

 예: 선형 회귀를 완전히 비선형 데이터에 쓴 경우

   데이터 모양:        모델이 그은 직선:
       •
        •                    ___
      •  •                 _/
       •  •              _/
      •    •           _/
       •    •        _/

 훈련 오차: 높음 (35%)
 테스트 오차: 높음 (36%)

 특징: 훈련과 테스트 오차가 비슷하게 높다

 진단 방법: 훈련 오차가 이미 높으면 → Bias 문제

 해결책: 모델을 복잡하게
 - 더 높은 차수 다항 회귀 써보기
 - 신경망 사용
 - 깊은 나무 허용

 [고분산 모델 (High Variance)]

 예: 깊은 의사결정나무

   데이터 모양:        나무가 그은 경계:
       •
        •               ∧∨∧∨∧∨∧∨∧
      •  •              ∨∧∨∧∨∧∨∧∨
       •  •
      •    •

 훈련 오차: 낮음 (5%)
 테스트 오차: 높음 (40%)

 특징: 훈련과 테스트 오차의 격차가 크다

 진단 방법: 훈련 오차는 낮은데 테스트가 높으면 → Variance 문제

 해결책: 모델을 단순하게
 - 깊이 제한
 - 더 많은 데이터 수집
 - 정규화 추가
 - 앙상블 방법 사용

 [실제 시나리오]

 '모델 정확도가 떨어졌어요. 어떻게 해야 하나요?'

 먼저 진단:
 훈련 오차 낮음 + 테스트 오차 높음?
 → Variance 문제 → 단순화해라

 훈련 오차 높음 + 테스트 오차도 높음?
 → Bias 문제 → 복잡화해라

 이 진단을 잘못하면,
 틀린 해결책을 쓰게 돼요.

 의사결정나무의 경우:

 '깊은 나무를 만들었는데
  테스트 성능이 안 좋아요'

 → Variance 문제
 → 깊이를 줄여라 (max_depth 파라미터)
 → 그럼 안정적이 된다

 반대로,

 '얕은 나무를 만들었는데
  훈련도 못 맞춰요'

 → Bias 문제
 → 깊이를 늘려라
 → 그럼 학습이 된다"
```

→ **실무에서는**: sklearn에서 max_depth 파라미터를 조정할 때, 학습 곡선을 보면서 Bias/Variance 균형을 찾습니다.

---

### → 슬라이드 3-09: 해결 전략 예고

```
"그런데 여기서 문제가 있어요.

 '복잡하게 만들고 싶은데,
  Variance가 너무 올라간다'

 또는

 '단순하게 만들고 싶은데,
  Bias가 너무 올라간다'

 이런 딜레마에 빠져요.

 '더 좋은 방법이 있을까?'

 오늘 나머지 시간의 핵심 아이디어가 이겁니다.

 [핵심 아이디어]

 '한 나무가 불안정하다면,
  여러 나무를 심어서
  평균을 내면 어떨까?'

 예:

 나무 1의 예측: 이탈 확률 90%
 나무 2의 예측: 이탈 확률 75%
 나무 3의 예측: 이탈 확률 85%
 ...
 나무 100의 예측: 이탈 확률 80%

 평균: (90+75+85+...+80)/100 = 82%

 흥미로운 사실:

 '개별 나무는 80%정도의 정확도'
 '하지만 100개 나무의 평균은 95% 정확도'

 왜?

 각 나무의 오류가 '무작위'라서,
 평균 내면 오류가 상쇄되기 때문입니다.

 여론조사 비유:

 1명에게 물어봐요:
 '너는 이 정책 찬성?'
 → 그 사람의 의견은 편향될 수 있음

 1,000명에게 물어봐요:
 → 개인의 편향이 상쇄됨
 → 더 신뢰할 수 있음

 이게 Bagging이고, Random Forest의 원리입니다.

 이제 이 방법을 배웁시다."
```

→ **실무에서는**: Random Forest는 온라인 광고 타겟팅, 신용카드 이상거래 탐지 등에 가장 널리 쓰이는 모델입니다.

---

## [해결] 앙상블 — Bagging (33~45분)

### → 슬라이드 3-10: Bagging의 철학

```
"Bagging이 뭘까요?

 Bootstrap + Aggregating의 합성어예요.

 한국식으로는 '부트스트랩 집계'정도로 볼 수 있어요.

 철학은 간단합니다:

 '병렬로 여러 모델을 만들어서
  그 결과를 평균 낸다'

 과정:

 [1단계] 원본 데이터 (1,000명)

 [2단계] Bootstrap: 복원 추출로 다양한 데이터셋 만들기

 데이터셋 1: 1,000명 (무작위로 뽑음)
 데이터셋 2: 1,000명 (무작위로 뽑음)
 ...
 데이터셋 100: 1,000명 (무작위로 뽑음)

 특징: 같은 사람이 여러 번 뽑힐 수 있다.

 [3단계] 각 데이터셋으로 모델 학습

 모델 1: 데이터셋 1로 의사결정나무 학습
 모델 2: 데이터셋 2로 의사결정나무 학습
 ...
 모델 100: 데이터셋 100으로 의사결정나무 학습

 [4단계] Aggregating: 결과 합치기

 새 고객 X가 들어옴:
 모델 1: 이탈 확률 85%
 모델 2: 이탈 확률 72%
 ...
 모델 100: 이탈 확률 88%

 최종 예측: (85+72+...+88)/100 = 81%

 [효과]

 '개별 모델의 정확도: 75~80%'
 '앙상블 모델의 정확도: 92~95%'

 왜 이렇게 효과가 클까?

 각 나무가 서로 다른 부분에서 실수해요.

 나무 1은 고소득층에서 틀리고,
 나무 2는 저소득층에서 틀리고,
 ...

 평균을 내면, 이런 실수들이 상쇄되는 거죠.

 수학적으로:

 독립인 확률변수 n개의 평균 분산 = σ²/n

 나무 100개를 평균 내면 분산이 1/100으로 줄어든다는 뜻이에요.

 (단, 나무들이 독립일 때)

 [비유]

 '한 명의 전문가 vs 100명의 평민의 투표'

 전문가 1명: 정확할 수도, 틀릴 수도
 평민 100명: 개인은 부정확하지만, 투표 결과는 믿을 만함

 이게 집단 지성(Collective Intelligence)입니다."
```

→ **실무에서는**: Netflix의 추천 알고리즘도 내부적으로 수백 개의 나무를 앙상블해서 사용합니다.

---

### → 슬라이드 3-11: Bootstrap Sampling

```
"Bootstrap이 정확히 뭔지 이해해봅시다.

 [복원 추출 (Sampling with Replacement)]

 주머니에 공 5개가 있어요: [A, B, C, D, E]

 일반적 추출 (비복원):
 - 1번 뽑기: B 뽑음 → 주머니에 4개 남음
 - 2번 뽑기: D 뽑음 → 주머니에 3개 남음
 - 3번 뽑기: A 뽑음 → 주머니에 2개 남음
 결과: [B, D, A]

 Bootstrap (복원):
 - 1번 뽑기: B 뽑음 → 다시 집어넣음 → 주머니에 여전히 5개
 - 2번 뽑기: B 뽑음 (또 B!) → 다시 집어넣음
 - 3번 뽑기: A 뽑음 → 다시 집어넣음
 결과: [B, B, A]

 이상하게 들릴까요?

 하지만 이게 'diversity'를 만들어요.

 각 Bootstrap 샘플이 서로 다르거든요.

 [데이터 관점에서]

 원본: 1,000명의 고객

 Bootstrap 샘플 1: 1,000명 (중복 허용)
 → 어떤 고객은 0번, 어떤 고객은 3번 뽑혔을 수도

 Bootstrap 샘플 2: 1,000명 (중복 허용)
 → 다른 구성

 ...

 Bootstrap 샘플 100: 1,000명 (중복 허용)
 → 또 다른 구성

 [효과]

 각 샘플이 조금씩 달라서,
 각 샘플에서 학습한 나무가 조금씩 달라요.

 예:
 나무 1: '고소득 남성이 이탈 위험'
 나무 2: '저소득 여성이 이탈 위험'
 나무 3: '장기 고객일수록 이탈 위험'
 ...

 다양한 관점이 생기는 거죠.

 [이론적 배경]

 '같은 분포에서 나온 여러 샘플은,
  평균을 내면 원래 분포에 가까워진다'

 (대수의 법칙, Law of Large Numbers)

 이게 Bootstrap의 기반입니다.

 '원본 데이터가 모집단의 좋은 대표라고 가정하면,
  원본에서 복원 추출한 샘플도
  거의 비슷한 분포를 가진다'

 따라서:
 '원본으로 여러 샘플을 만들어서
  각각 모델을 학습한 후
  평균 내면
  편향이 줄어들지는 않지만
  분산은 확실히 줄어든다'"
```

→ **실무에서는**: sklearn의 BaggingClassifier가 자동으로 이 과정을 합니다. 우리는 그냥 n_estimators만 지정하면 돼요.

---

### → 슬라이드 3-12: 랜덤 포레스트: 나무에서 숲으로

```
"Bagging까지는 '같은 데이터에서 여러 샘플로 나무를 만드는 것'이었어요.

 하지만 여기서 한 가지 더합니다:

 '변수도 무작위로 선택한다'

 이게 Random Forest입니다.

 [일반적 의사결정나무]

 분할할 때:
 - 모든 변수 검토 (변수 A, B, C, D, E...)
 - 가장 좋은 변수 선택

 [Random Forest의 각 나무]

 분할할 때:
 - 전체 변수 중 일부만 랜덤 선택 (예: A, C, E만)
 - 그 중에서 가장 좋은 변수 선택

 [효과]

 '같은 샘플인데도 각 나무가 보는 변수가 달라서
  나무들이 더 다양해진다'

 예:
 나무 1: A, C, E를 봤어 → 규칙: A > 5이면 이탈
 나무 2: B, D, E를 봤어 → 규칙: B < 10이면 이탈
 나무 3: A, B, F를 봤어 → 규칙: F가 높으면 이탈

 다양한 규칙이 나온다!

 수학적으로:

 Bagging: 분산 감소 = σ²/(n × 상관계수)
 Random Forest: 분산 감소 = σ²/n (상관계수 더 작음)

 변수 다양성이 나무 간 상관을 낮추니까,
 분산 감소 효과가 더 크다는 뜻입니다.

 [하이퍼파라미터]

 n_estimators: 나무 몇 개? (보통 100~500)
 max_depth: 각 나무 깊이 (보통 5~20)
 min_samples_split: 분할에 필요한 최소 샘플 수
 max_features: 각 분할에서 고려할 변수 수

 일반적 설정:

 max_features = sqrt(전체 변수 수)

 예: 변수 16개면, 각 분할마다 4개만 보게 됨.

 이렇게 제약을 주면 나무들이 다양해져요."
```

→ **실무에서는**: Random Forest는 대부분의 tabular data 대회에서 baseline입니다. 간단하고, 잘 작동하고, 해석이 어느 정도 가능하기 때문이죠.

**[예상 질문]** "변수를 무작위로 선택하면 중요한 변수를 놓칠 수도 있지 않나요?"
→ "네, 개별 나무는 놓칠 수 있어요. 하지만 100개 나무를 평균 내면, 중요한 변수는 대부분의 나무에서 고르게 사용돼서 중요도에 반영됩니다."

---

### → 슬라이드 3-13: 변수 중요도

```
"Random Forest의 큰 장점 중 하나가

 '어떤 변수가 중요한가'를 알려준다는 거예요.

 [원리]

 각 분할에서:
 'Gini를 얼마나 줄이는가'를 측정합니다.

 나무 1:
 - 첫 분할에서 변수 A 사용, Gini 0.5 → 0.3 (0.2 감소)
 - 두 번째 분할에서 변수 B 사용, Gini 0.3 → 0.15 (0.15 감소)
 - 세 번째 분할에서 변수 C 사용, Gini 0.15 → 0.08 (0.07 감소)

 나무별 변수 중요도:
 A: 0.2
 B: 0.15
 C: 0.07
 (D, E, F는 0)

 이를 100개 나무 전체에서 평균내면,

 최종 변수 중요도:
 A: 0.35 (가장 중요)
 B: 0.22
 C: 0.18
 D: 0.15
 E: 0.08
 F: 0.02

 [해석]

 '이 모델에서 고객 이탈을 예측하는 데 가장 중요한 변수는?'
 → A (35%), 그 다음 B (22%)

 [실무 적용]

 '우리 비즈니스에서 고객 이탈을 막으려면
  어느 부분에 집중해야 하나요?'

 → 변수 A를 개선하는 데 집중해라.
 → 변수 A가 고객 이탈의 35%를 설명하니까.

 [주의]

 변수 중요도는 '예측에서의 기여도'이지,
 '실제 인과관계'는 아닙니다.

 변수 A가 높을수록 이탈이 많다고 해도,
 변수 A가 이탈의 '원인'은 아닐 수 있어요.

 예:
 변수 A가 '고객 불만 지수'라면,
 A는 이탈의 '신호(signal)'일 뿐
 '근본 원인'은 '서비스 품질'일 수도.

 이 구분이 5차시의 핵심입니다."
```

→ **실무에서는**: 마케팅팀이 "어느 고객 세그먼트에 투자해야 할까"를 결정할 때, 변수 중요도를 보고 판단해요.

---

## [실무] 나무가 실제로 하는 일 (45~53분)

### → 슬라이드 3-14: 대출 심사 규칙

```
"이제 구체적인 실무 사례를 봅시다.

 [사례 1] 은행의 대출 심사

 은행이 해야 할 일:
 '이 고객에게 대출을 해줄까? 말까?
  그리고 금리는 얼마로 할까?'

 과거 방식:
 '신용등급, 소득, 근속년수를 보고
  금융감독위원회가 정한 기준표로 판단한다'

 새로운 방식:
 의사결정나무로 만든 규칙:

 IF 연소득 < 3,000만 AND 연체 이력 있음
 THEN 대출 거절

 ELSE IF 연소득 >= 3,000만 AND 부채비율 > 60%
 THEN 금리 3% 인상

 ELSE IF 연소득 >= 5,000만 AND 근속년수 > 5년
 THEN 금리 0.5% 인하

 ELSE
 THEN 표준 금리 적용

 [장점]

 1) 명확하다: '왜 이 고객이 거절됐는가' 한눈에 알 수 있음

 2) 설명 가능하다: 고객 민원 시 '이 규칙 때문'이라고 설명 가능

 3) 규제 준수: 금융감독위가 '설명 가능성'을 요구해서,
               의사결정나무가 표준

 4) 속도: 한 번에 규칙만 확인하면 돼서 엄청 빠름

 [한계]

 하나의 의사결정나무로는 불안정하다는 게 드러났어요.

 '지난달은 이 고객을 승인했는데
  이달은 거절한다?'

 고객 민원이 쏟아져요.

 해결책: Random Forest로 바꾼다

 100개 나무의 '평균 점수'가 0.6점 이상이면 승인
 0.4점 이하면 거절
 0.4~0.6 사이면 추가 심사

 이렇게 하면 '확률 기반 판단'이 돼서 안정적입니다."
```

→ **실무에서는**: 미국 대출 시장의 약 70%가 의사결정나무 기반 신용평점 모델을 사용합니다 (FICO 스코어와 유사한 방식).

---

### → 슬라이드 3-15: 이상거래 탐지

```
"[사례 2] 신용카드 이상거래 탐지

 은행 카드 부서의 문제:
 '이 거래가 정상인가? 사기인가?'

 과거 방식:
 '규칙 기반'
 - 거래액 > 100만 원이면 알림
 - 해외 거래면 알림
 - ... (수십 개 규칙)

 문제:
 - 정상 거래도 막힘 (오탐)
 - 실제 사기도 못 잡음 (미탐)

 새로운 방식:
 Random Forest (의사결정나무 100개)

 입력:
 - 거래액
 - 거래지역
 - 시간대
 - 평상시 소비 패턴과의 편차
 - 카드 이용 이력
 - ...

 출력:
 사기 확률

 100개 나무의 투표:
 나무 1: 사기 (80% 확률)
 나무 2: 정상 (60% 확률)
 나무 3: 사기 (75% 확률)
 ...
 나무 100: 사기 (82% 확률)

 평균: 사기 확률 77%

 규칙:
 77% > 70% → SMS 인증 요청

 [효과]

 '정상 거래 오탐: 5% → 1%'
 '사기 탐지율: 75% → 94%'

 한 나무로는 불가능했던 성능입니다.

 왜?

 데이터 노이즈가 많거든요.

 정상 거래도 때로 이상하게 보이고,
 사기도 때로 정상처럼 보여요.

 한 나무는 그 노이즈에 흔들려서
 '이번 월요일은 이탈이라고, 저번 월요일은 아니라고' 판단해요.

 100개 나무를 평균 내면 노이즈가 상쇄돼요.

 [비용 효과]

 기존 방식: 사람이 일일이 검토
 → 검토팀 200명 필요 → 연 200억 원 비용

 Random Forest 방식: 자동 판단
 → 검토팀 10명만 필요 (예외 케이스만)
 → 연 20억 원 비용
 → 정확도도 더 높음

 이게 기술이 비즈니스 영향을 미치는 방식입니다."
```

→ **실무에서는**: 비자카드, 마스터카드 같은 글로벌 결제사들이 모두 Random Forest 기반의 사기 탐지 시스템을 운영합니다.

**[예상 질문]** "의사결정나무의 규칙을 직접 쓰지 않고 왜 Random Forest의 확률을 쓰나요?"
→ "의사결정나무 1개는 불안정해서 같은 데이터도 다르게 판단할 수 있어요. Random Forest는 100개 나무가 투표하기 때문에 일관성이 있습니다."

---

### → 슬라이드 3-16: 랜덤 포레스트의 한계

```
"Random Forest는 훌륭한 도구지만, 한계가 있어요.

 [한계 1] 해석력 감소

 의사결정나무: 규칙을 읽을 수 있음
 'IF A > 5 THEN 이탈'

 Random Forest: 확률만 줌
 '이탈 확률 73%'
 → 왜? 모름. 100개 나무의 투표니까.

 규제 관점에서는 문제 있어요.
 '왜 이 고객을 거절했나요?'
 → 'Random Forest가 73% 사기라고 했어서요'
 → '구체적인 이유는?'
 → '... 설명 못 합니다'

 [한계 2] 속도

 나무 1개: 1ms 판단
 나무 100개: 100ms 판단
 나무 1,000개: 1,000ms (1초!)

 실시간 거래가 필요한 상황에서는 느릴 수 있어요.

 [한계 3] 순차 패턴을 못 잡음

 Random Forest는 '시간 정보'를 무시합니다.

 예: '작년 이맘때 이 고객은 항상 휴가를 갔다'
     → 시간 정보 필요

 하지만 Random Forest는 그냥 변수 값만 보니까
 '이번 1월은 특별한 거래가 많네?'
 라고 판단해서 사기로 탐지할 수도.

 또 다른 예: '주가 차트'
 시간에 따른 패턴이 있는데,
 Random Forest는 각 시점을 독립적으로 봐요.

 '이 시점의 주가를 예측해줘'
 '작년 같은 시점의 주가는 뭐였지?'
 → 관계없음. Random Forest는 그냥 '지금 이 값들'만 봐요.

 이런 한계들을 보완하는 게 다음 차시 (4차시)입니다.

 GBM과 XGBoost는:
 - 순차 학습으로 오류를 점진적으로 줄인다
 - 더 해석 가능한 특성 중요도를 제공한다
 - 속도를 더 최적화한다

 하지만 그건 다음 시간 이야기고,

 지금은 Random Forest의 기본 개념을 확실히 이해하는 게 중요합니다."
```

→ **실무에서는**: Random Forest의 한계를 보완하기 위해 XGBoost로 전환하는 회사들이 많아요. Kaggle 대회에서 우승팀의 80%가 XGBoost를 사용합니다.

---

## [마무리] (53~60분)

### → 슬라이드 3-17: 핵심 3줄 요약

```
"오늘 배운 것을 정리하면:

 [핵심 1] 나무의 직관
 → '데이터를 순수한 그룹으로 나눈다'
 → 규칙이 명확해서 해석 가능
 → 하지만 불안정하다

 [핵심 2] Bias-Variance Tradeoff
 → '모든 모델 선택의 나침반'
 → 과적합(높은 분산) vs 과소적합(높은 편향)의 균형
 → 훈련/테스트 오차 차이로 진단한다

 [핵심 3] 집단 지성의 힘
 → 'Bootstrap + Aggregating = Bagging'
 → '변수 다양성을 더하면 = Random Forest'
 → 개별 나무의 약점을 앙상블로 극복한다

 [다음 차시 예고]

 지금까지 우리가 배운 앙상블 방식:
 - Bagging: 병렬로 많은 모델을 만들어서 평균낸다
 - Random Forest: 각 모델이 다른 변수를 봐서 다양성 확보

 다음 차시 (4차시)에서는:
 'Boosting: 순차적으로 모델을 만들어서 틀린 부분을 집중 공략한다'

 이 방식의 극단적 형태가 XGBoost예요.

 'Kaggle 우승팀 80%가 쓰는 모델'

 그리고 거기서 한 가지 더 배웁니다:

 '모델이 아무리 정확해도, 설명할 수 없으면 쓸 수 없다'

 5차시에서 SHAP로 블랙박스를 열어보면,
 여러분은 '어떤 모델도 설명할 수 있게' 됩니다.

 이것이 2026년 ML 엔지니어의 필수 기술입니다."
```

→ **실무에서는**: XGBoost 없이 현대 데이터 과학팀은 존재하지 않습니다. 정말 표준입니다.

---

### → 슬라이드 3-18: 예고: 나무 하나로도 한계가 있다

```
"마지막 당부:

 지금까지의 여정을 보면:

 1차시: '질문을 던질 줄 알아야 한다'
 2차시: '과적합을 주의해야 한다'
 3차시 (오늘): '한 모델의 약점을 앙상블로 극복한다'

 그런데 여기서도 한계가 있어요.

 Random Forest의 정확도가 95%라고 해도,
 '정말 이 모델을 신뢰할 수 있을까?'
 '이 95%는 어디서 나온 걸까?'

 이 질문이 중요합니다.

 다음 2차시 (4~5차시)에서는:
 'XGBoost로 정확도를 98%까지 높이는 방법'
 'SHAP로 그 98%의 근거를 설명하는 방법'

 을 배웁니다.

 그리고 가장 중요한 깨달음:

 '높은 정확도 ≠ 좋은 모델'

 정확도 99%지만 '왜 이렇게 예측했는지' 설명 못 하는 모델
 vs
 정확도 92%이지만 '왜인지' 명확하게 설명할 수 있는 모델

 비즈니스에서 어느 게 더 가치 있을까요?

 상황에 따라 다릅니다.

 하지만 금융, 의료, 법무, 인사 같은 곳에서는
 후자가 훨씬 더 가치 있어요.

 '신뢰'가 거기 있기 때문이죠.

 이 균형을 찾는 게, 5차시까지의 여정입니다.

 오늘은 여기까지.

 다음 시간에 뵙겠습니다."
```

→ **실무에서는**: 모델 배포 심사에서 '정확도'보다 '설명 가능성'이 거절 사유가 되는 경우가 많아요.

---

## [부록] 60분 타임라인

```
0~3분:    [연결] 2차시 → 3차시 브리지
          - 3-01: 직선이 못 잡는 패턴들

3~21분:   [개념] 의사결정나무
          - 3-02: 스무고개로 분류한다
          - 3-03: IF-THEN 규칙 구조
          - 3-04: 어떻게 나눌 것인가 (Gini 불순도)
          - 3-05: 의사결정나무의 강점
          - 3-06: 의사결정나무의 약점 (불안정성)

21~33분:  [핵심] Bias-Variance Tradeoff
          - 3-07: Bias-Variance 2×2 (과녁 비유)
          - 3-08: 고편향 vs 고분산 모델
          - 3-09: 해결 전략 예고

33~45분:  [해결] 앙상블 — Bagging
          - 3-10: Bagging의 철학 (병렬 학습)
          - 3-11: Bootstrap Sampling (복원 추출)
          - 3-12: 랜덤 포레스트: 나무에서 숲으로
          - 3-13: 변수 중요도

45~53분:  [실무] 나무가 실제로 하는 일
          - 3-14: 대출 심사 규칙 (의사결정나무)
          - 3-15: 이상거래 탐지 (Random Forest)
          - 3-16: 랜덤 포레스트의 한계

53~60분:  [마무리]
          - 3-17: 핵심 3줄 요약
          - 3-18: 예고 — 나무 하나로도 한계가 있다
```

---

## 작성 원칙 준수 체크

- [x] 슬라이드 번호 명시 (3-01 ~ 3-18 형식)
- [x] 2차시 연결 자연스럽게 ("직선으로 안 되는 세상" → "그럼 뭘 하나?")
- [x] 의사결정나무 개념을 스무고개 비유로 직관화
- [x] Bias-Variance를 과녁 비유로 설명 (수식 제외)
- [x] Bias-Variance 전환이 부드러워짐 (나무의 불안정성 → Bias-Variance → 해결책)
- [x] 수식 완전 제거 (Gini는 직관만, "섞여있으면 높음" 정도)
- [x] 각 슬라이드 후 "→ 실무에서는" 한 줄 포함 (모두 18개)
- [x] [예상 질문] 태그 포함 (6개)
- [x] 18장 구조로 최적화 (기존 22장 통합)
- [x] 톤: 직관 → 원리 → 실무의 3단계
- [x] 60분 타임라인 명시
- [x] 4차시 예고 (XGBoost + SHAP)
- [x] 강사의 경험담 톤 (자연스러운 설명)
