# 5차시 강의 대본 — 설명할 수 있어야 쓸 수 있다

> 시간: 60분 | 대상: ML 입문자 (베이스 혼합) | 톤: 설득형 + 체험 중심

---

## [연결] 4차시 → 5차시 브리지 (3분)

### 오프닝 훅
```
"지난 시간 마지막에 이런 질문을 남겼습니다.
 '잘 맞추면 끝인가?'

 네, 끝이 아닙니다.
 세 가지가 더 있습니다.

 오늘 이 세 가지를 건드리면,
 여러분은 모델을 돌리는 것에서 끝내지 않고,
 그 모델을 누군가에게 설득할 수 있게 됩니다."
```

**[전환 포인트]**
```
"정확도 99%가 나왔는데, 왜 설명을 해야 하는 걸까요?
 왜 데이터가 공정한지 확인해야 하는 걸까요?
 왜 상관관계를 인과관계로 착각하면 안 되는 걸까요?

 이 세 가지가 '실무 모델'과 '학습 모델'을 가르닙니다."
```

---

## [1부] 설명할 수 있어야 쓸 수 있다 (20분)

### → 슬라이드 5-01: ML의 3가지 한계

```
"XGBoost가 정확도 95%를 냈다고 합시다.
 대기업 신용 심사팀에 보고합니다.

 담당자 첫 질문: '왜 이 고객을 거절했어요?'

 여기서 우리는 막힙니다.
 XGBoost는 정확하지만, 답을 해주지 않거든요.
 이게 첫 번째 한계 — 블랙박스입니다."
```

**[세 가지 한계 나열]**
```
한계 1: 블랙박스 — 왜 이렇게 예측했는지 설명 불가
한계 2: 편향 — 편향된 데이터로 학습하면 편향된 예측
한계 3: 상관 ≠ 인과 — 예측이 정확해도 이유를 몰라요

실무에서는 '정확한 예측'만으로는 부족합니다.
'왜 그렇게 예측했는지' '그 이유가 공정한지' '그 이유가 인과는 맞는지'
이걸 설명할 수 있어야 비로소 쓸 수 있는 모델이 됩니다."
```

→ **실무 적용**: 금융·의료·법무에서는 법으로 설명을 요구합니다.
   개인정보보호법, EU GDPR 등이 "모델이 왜 그 결정을 했는지 설명하시오"를 강제합니다.

**[예상 질문]** "그럼 딥러닝은 더 복잡하니까 더 설명이 안 되겠네요?"
→ "맞습니다. 그래서 금융·의료는 의도적으로 해석 가능한 모델(XGBoost)을 쓰거나,
   설명 기술(SHAP)을 함께 사용합니다."

---

### → 슬라이드 5-02: SHAP — 게임이론으로 범인을 지목한다

```
"그럼 XGBoost의 검은 상자를 열 수 있을까요?
 네, SHAP이라는 도구가 있습니다.

 SHAP은 각 변수가 최종 예측에 얼마나 기여했는지
 게임이론으로 계산합니다.

 예를 들어, 이 고객의 이탈 확률이 80%라면:
   나이(30) → +15% 기여
   월 사용료(5만원) → +20% 기여
   지난 3개월 서비스 불만 3건 → +45% 기여

 최종 예측 80% = 기본 확률 45% + 각 변수의 기여도 합산

 이제 담당자가 고객한테 설명할 수 있어요:
 '당신은 그 불만 때문에 이탈 위험이 높아 보입니다.
  이 부분을 개선하면 확률이 내려갈 겁니다.'"
```

→ **실무 적용**: 신용 심사, 의료 진단, 채용 AI 모두 이 설명이 필수입니다.

**[비유]**
```
Feature Importance: "전교 회장 뽑을 때, 전교적으로 가장 인기 많은 학생이 누구인가?"
               → 전체에서 누가 중요한가

SHAP Value:   "이번 시험에서, 그 학생의 수학 점수가 성적에 얼마나 기여했나?"
               → 이 사건에서 누가 책임인가
```

**[예상 질문]** "모든 변수의 SHAP을 다 계산해야 하나요? 시간이 오래 걸리진 않나요?"
→ "맞습니다. 변수가 1,000개면 계산이 무거워집니다.
   그래서 TreeSHAP이라는 특화 알고리즘이 있어서,
   XGBoost·RandomForest에서는 수 초 안에 계산이 완료됩니다."

---

### → 슬라이드 5-03: 워터폴 차트 읽기 (수식 없이 직관만)

```
"이 고객의 SHAP을 시각화하면 '워터폴 차트'가 나옵니다.

 [그래프 상상]

 왼쪽에 기본값: 평균 이탈 확률 45%

 오른쪽으로 향해, 각 변수가 위/아래로 올려집니다:
   '서비스 불만' → 빨간색 위로 +30%
   '월 사용료 많음' → 파란색 아래로 -10%
   '지난달 로그인 자주' → 파란색 아래로 -8%
   ...

 끝점 = 최종 예측값 80%

 이 그래프만 봐도:
 - '어떤 변수가 이탈을 가장 키웠는가' 한눈에 봄
 - '만약 이 변수를 개선하면?' 직관적으로 상상 가능
 - '왜 이 고객인가' 설명하기 쉬움"
```

→ **실무 적용**:
   - 고객 이탈 위험군: "당신의 이탈 위험을 높이는 요소 3가지"
   - 대출 거절: "거절 결정에 영향 미친 요소 순서대로"
   - 암 진단 보조: "이 환자가 고위험인 이유"

**[강조]**
```
"이 차트가 좋은 이유는:
 복잡한 수학을 몰라도
 '이게 맞는지' 판단할 수 있기 때문입니다.

 '불만이 많으니까 이탈이 높다' — 당연한 결과니까 믿을 수 있어요.

 만약 차트에 이상한 게 나오면?
 '음수 점수가 길수록 좋아져야 하는데,
  나이가 올라갈수록 이탈 위험이 커진다?'

 그럼 그건 진짜 이상한 거고,
 모델을 다시 확인해야 한다는 신호입니다."
```

---

### → 슬라이드 5-04: 편향 실사례 — Amazon 채용 AI

```
"2018년 Amazon이 채용 AI를 만들었습니다.
 문제는 이 AI가 여성 지원자를 체계적으로 거절했다는 거에요.

 왜였을까요?

 Amazon의 과거 데이터를 보면 — 엔지니어직에 남성 80%, 여성 20%.

 AI는 이 데이터를 그대로 학습했어요.
 '과거에 남성이 많이 채용됐으니까, 남성이 좋은 신호겠지'

 그리고 이 신호를 유지하려고 학습합니다.

 결과:
 여성 지원자가 남성보다 더 뛰어나도,
 AI는 '이건 통계적 이상'이라고 판단하고 깎아내렸어요.

 이게 데이터 편향입니다."
```

→ **실무 적용**:
   - 신용대출 AI: 과거 차별 영향력이 모델에 그대로 반영
   - 의료 AI: 소수 집단의 데이터 부족 → 그들에게 낮은 정확도
   - 추천 시스템: 과거 추천 이력이 미래 선택을 좌우

**[SHAP으로 차별을 감지하는 방법]**
```
"이런 문제를 어떻게 찾을까요?
 SHAP으로 찾을 수 있어요.

 여성 지원자 100명의 '성별' 변수 SHAP을 보면:
 '성별' → 평균 -15% (거절 방향)

 남성 지원자 100명:
 '성별' → 평균 +12% (승인 방향)

 같은 변수가 다른 그룹에서 다르게 작용한다?
 그건 불공정의 신호입니다.

 이걸 발견했으니 이제 모델을 고칠 수 있어요:
 - '성별' 변수를 아예 뺄 수도 있고
 - '성별'의 영향력을 제한할 수도 있고
 - 과거 데이터를 더 균형 있게 샘플링할 수도 있어요"
```

**[예상 질문]** "성별을 빼면 또 다른 변수가 대신하지 않나요?"
→ "정확한 지적입니다. 이건 '기술'만으로 안 되고,
   데이터 수집 단계부터 설계를 다시 해야 합니다.
   과거 차별을 지속하는 데이터 수집을 바꾸는 것.

   근본 원인을 모르면, 변수를 아무리 바꿔도
   그 영향이 다른 형태로 나타나요."

---

### → 슬라이드 5-05: 시각화 설득 원칙 — 주장→근거→시각화 3단

```
"마지막 한 가지 — 설득하는 방법.

 여러분이 모델을 만들고 분석을 했으면,
 누군가에게 '이게 맞아'를 증명해야 해요.

 설득의 3단계 구조:

 [1단] 주장: 결론을 먼저 말한다
       "우리 고객 이탈이 심각하게 증가했습니다"

 [2단] 근거: 숫자로 뒷받침한다
       "3월 이탈율 12% → 4월 18% → 5월 24%"

 [3단] 시각화: 눈으로 확인하게 한다
       꺾은선 그래프로 상승 추세를 한눈에
       → 신뢰성 폭발"
```

**[차트 선택 원칙]**
```
"비교하고 싶으면    → 막대 또는 꺾은선 (시간 흐름이면 꺾은선)
 관계를 보고 싶으면  → 산점도 (두 변수가 어떻게 연관되나)
 구성을 보고 싶으면  → 누적 막대 (파이는 3개 이상이면 오히려 헷갈림)
 분포를 보고 싶으면  → 히스토그램, 박스플롯"
```

**[흔한 실수 vs 올바른 표현]**

```
❌ "전처리 후 데이터 품질이 좋아졌습니다"
✓ "결측치 비율이 8.3% → 0%로 줄었고,
   이상치(음수 대여량) 제거 후 평균이 42 → 38로 감소했습니다"

❌ "모델 성능이 향상됐습니다"
✓ "정확도가 82% → 91%로 9%p 개선, AUC는 0.71 → 0.85"

❌ "변수가 중요합니다"
✓ "서비스 불만 이력이 이탈 예측의 45%를 설명합니다(SHAP 평균)"
```

**[핵심 규칙]**
```
"숫자로 증명하지 않으면 주장이 아니라 의견입니다.

 의견은 반박할 수 없지만,
 '3월 12% → 4월 18%'는 사실이에요.

 이 사실 앞에는 이견이 없습니다."
```

→ **실무 적용**:
   - 경영진 보고: 숫자 없는 분석은 버려집니다
   - 모델 감사(Audit): 근거를 요구합니다
   - 논문 발표: 수치 없는 주장은 기각됩니다

**[예상 질문]** "수치가 많으면 너무 복잡하지 않나요?"
→ "맞습니다. 청중에 따라 다릅니다.
   임원진: '3월 12% → 4월 18%' 1줄만
   담당팀: 월별 상세 수치
   학술발표: 통계 검정까지

   '누가 듣는가'에 따라 디테일 수준을 조절하세요."

---

## [2부] 데이터가 먼저다 (15분)

### → 슬라이드 5-06: EDA 왜 먼저인가 — "쓰레기 IN → 쓰레기 OUT"

```
"모델을 바로 돌리고 싶겠죠.
 'XGBoost 깔아서 정확도 90% 나왔어!'

 하지만 그 전에 물어봐야 할 게 있어요:
 '데이터가 정말 깨끗한가?'

 데이터가 거진다면 — 아무리 좋은 모델도 거진 결과가 나옵니다.

 이게 '쓰레기 IN, 쓰레기 OUT' 원칙입니다.

 비유:
 아무리 좋은 카메라로 찍어도,
 렌즈가 더럽거나 필름이 손상되면
 사진이 망합니다.

 모델이 카메라고, 데이터가 렌즈·필름인 거죠."
```

→ **실무 적용**:
   실무 데이터 프로젝트의 70~80%는 데이터 정제에 시간을 씁니다.
   모델 개발은 20~30%에 불과합니다.

**[예상 질문]** "그럼 EDA는 얼마나 오래 해야 하나요?"
→ "규칙은 없지만, 새 데이터면 1~2주, 익숙한 도메인이면 2~3일 정도.
   핵심은 '문제가 없을 때까지' 확인하는 거예요."

---

### → 슬라이드 5-07: 진단 체크리스트 — 결측/이상치/분포/관계

```
"EDA는 네 가지를 확인합니다:

 1) 결측치 — 값이 빠진 부분이 있나?
 2) 이상치 — 터무니없는 값이 있나?
 3) 분포 — 데이터가 어떻게 퍼져 있나?
 4) 관계 — 변수들이 어떻게 연관되나?

 이 네 가지가 정상이면,
 모델을 돌릴 자격이 생기는 거예요."
```

→ **실무 적용**: 이 체크리스트를 기억했다면, 데이터 분석의 절반을 푼 것입니다.

---

### → 슬라이드 5-08: 결측치 처리 의사결정 (5%/30% 기준)

```
"결측치가 나왔어요.
 어떻게 처리해야 할까요?

 규칙이 있습니다:

 [경우 1] 비율 < 5% + 무작위 분포
   → 행 삭제 (정보 손실 최소)
   → 예: 100행 중 3행만 결측 → 97행으로 학습

 [경우 2] 비율 5~30% + 패턴 있음
   → 그룹별 평균/중앙값으로 대체
   → 예: 시간대 변수에 시간대별 평균으로 채우기
        요일 변수에 요일별 중앙값으로 채우기

 [경우 3] 비율 > 30%
   → 변수 삭제 검토 또는 '결측' 자체를 범주로 추가
   → 예: '배송 주소 결측'은 '해외 고객'의 신호일 수도 있음
        → 삭제하지 말고 '결측=1, 있음=0' 이진 변수로 변환
"
```

**[강조]**
```
"처리 후 반드시 기록하세요:
 '변수 X: 결측 12% → 요일별 중앙값 대체'
 '이유: 요일별 패턴 발견됨'

 왜? 나중에 '왜 이렇게 했어?'라고 물어봤을 때
 답할 수 있어야 하고,
 감사(Audit)에서 근거를 요구할 수 있거든요."
```

→ **실무 적용**: 결측 처리는 '모델 변경'과 같습니다. 문서화는 필수입니다.

**[예상 질문]** "결측치를 예측 모델로 채우면 안 되나요?"
→ "할 수 있지만, 그건 고급 기법이고 위험합니다.
   왜냐면 다른 변수를 이용해 결측을 '만드는' 것이 되는데,
   그것이 원본 데이터보다 더 나을지 보장할 수 없거든요.

   입문자는 5%/30% 기준으로 충분합니다."

---

### → 슬라이드 5-09: 이상치의 두 얼굴 — 에러인가, 사건인가?

```
"박스플롯을 그렸어요.
 'Q1 - 1.5×IQR' 범위 밖의 점들이 여러 개 있어요.

 이게 이상치입니다.

 근데 여기서 헷갈리는 부분이 있습니다:
 이상치 = 제거해야 할 에러?
 이상치 = 중요한 사건?

 같은 점이 다르게 의미될 수 있어요."
```

**[구체적 예시]**

```
[경우 1] 에러 → 제거
  자전거 대여 데이터에서 '대여량 -50'이 나온다?
  음수는 불가능하니까 에러.
  → 즉시 제거 (입력 오류)

[경우 2] 사건 → 유지
  자전거 대여 데이터에서 '어느 날 대여량 2,500'
  일반적 평균 50인데 50배 높다.

  그 날이 뭐였나요?
  → 대형 스포츠 이벤트, 날씨 극단(폭우/폭설), 공휴일?

  그렇다면 이건 이상치가 아니라 '실제 사건'이에요.
  → 유지 (정보의 보물)

[경우 3] 측정 오류 → 클리핑(상한값 제한)
  온도 센서: -100도 (불가능, 센서 오류)
  센서의 최소값이 -50도니까 → -50으로 고정
  → 제거 대신 합리적 범위로 조정
```

→ **실무 적용**:
   - 금융: 거래량이 평소의 100배 → '구조적 변화'인지 '에러'인지 도메인 확인 필수
   - 의료: 환자 나이 200세 → 에러 (제거)
   - 마케팅: 고객 구매가 평소 10배 → 바이러스 마케팅 성공 (유지)

**[핵심 규칙]**
```
"이상치를 제거하기 전에, 항상 물어봐야 해요:
 '이게 진짜 불가능한 에러인가?
  아니면 드물지만 의미 있는 사건인가?'

 도메인 전문가(우리 경우 자전거 서비스 담당자)한테 물어봐야 합니다."
```

---

### → 슬라이드 5-10: Before/After 원칙 — 숫자로 증명하지 않으면 주장이 아니다

```
"EDA가 끝났습니다.
 결측 처리했고, 이상치도 처리했고, 파생변수도 만들었어요.

 '좋아졌다'고 보고하고 싶죠.

 근데 이렇게 하면 안 돼요:

 ❌ "데이터 품질이 좋아졌습니다"

 이건 의견이에요. 증명이 없어요.

 ✓ 이렇게 해야 해요:

 "전처리 전/후:
   - 결측치: 12% (285건) → 0% (완전 제거 또는 대체)
   - 이상치(음수 대여량): 8건 → 0건 (제거)
   - 평균 대여량: 47 → 51 (이상치 제거의 영향)
   - 표준편차: 84 → 52 (이상치 제거로 더 정상적)

  따라서 정제된 데이터가 통계적으로 안정적입니다."
```

**[Before/After 체크리스트]**
```
□ 같은 축을 사용했나? (동일 단위)
□ 숫자 기준을 명시했나? (결측 기준 5% 등)
□ 처리 전/후 샘플 수를 명시했나?
□ 처리 근거를 설명했나? (왜 이 방법을 선택했나)
```

→ **실무 적용**:
   - 분석 보고서: 항상 전처리 로그를 첨부합니다
   - 논문: Before/After 수표 없이는 기각
   - 감시(Audit): "왜 이 데이터를 빼셨죠?" → 근거를 요구

**[예상 질문]** "전처리 과정을 다 문서로 남기면 시간이 많이 걸리지 않나요?"
→ "처음엔 그렇습니다. 하지만 연습하다 보면:
   - Python에선 변수명이 처리 과정을 말해줍니다
   - 주석 하나만 남겨도 충분합니다
   - 실무에선 이 문서가 법적 근거가 될 수도 있습니다

   시간 투자의 가치가 충분해요."

---

### → 슬라이드 5-11: 과제 안내 — 서울 자전거 수요 시나리오 (1장)

```
"이제 직접 해볼 차례입니다.

 [시나리오]
 여러분은 서울시 따릉이 서비스의 데이터 분석가입니다.
 지난 3개월 일별 대여 데이터가 있어요.

 데이터에는:
 - 일자, 요일, 기온, 습도, 풍속, 강수, 휴일 여부
 - 일일 대여량

 문제: 결측, 이상치, 중복이 섞여 있습니다

 [과제]
 1) 결측치 탐색 및 처리 → 근거 기록
 2) 이상치 탐색 및 의사결정 → 전문가 (여러분)의 판단 근거 기록
 3) 분포 분석 → '어떻게 퍼져 있나' 시각화
 4) 파생변수 생성 → 시간대(아침/점심/저녁), 계절 등
 5) Before/After 비교 → 숫자로 증명

 [채점 기준]
 - 순위 없음 (경쟁 아님)
 - '왜 이 방법을 선택했는가'가 중요
 - 분석이 재현 가능한가 (random_state 고정)
"
```

→ **실무 적용**:
   데이터 분석 직무 면접에서 가장 자주 나오는 질문이
   "어떤 데이터를 어떻게 정제했는가"입니다.
   이 과제가 그 연습입니다.

---

## [3부] 당신의 다음 단계 (12분)

### → 슬라이드 5-12: 4단계 로드맵 — sklearn → XGBoost → SHAP → 에이전트

```
"지금까지 6주 동안 ML의 기초를 봤습니다.
 그 다음은 어디로 가야 할까요?

 4단계 로드맵이 있습니다:

 [1단계] sklearn으로 모델 직접 돌리기
   기간: 2주
   목표: LinearRegression / DecisionTree / RandomForest 실습
   과제: Kaggle 입문 대회 (Titanic)

   체크: 'train_test_split으로 분할하고 cross_val_score로 평가'가
        자동으로 손에서 나올 때까지

 [2단계] XGBoost + 하이퍼파라미터 튜닝
   기간: 3주
   목표: GridSearchCV / Optuna로 모델 최적화
   과제: Kaggle 초급~중급 대회

   체크: 'learning_rate와 max_depth 바꾸면 성능이 어떻게 변하는가'
        이걸 학습 곡선으로 설명할 수 있을 때

 [3단계] SHAP으로 모델 해석
   기간: 2주
   목표: shap.TreeExplainer, waterfall_plot, summary_plot
   과제: 내가 만든 모델을 SHAP으로 분석 → 블로그 포스팅

   체크: '이 고객이 이탈 위험인 이유'를 SHAP 워터폴로 설명할 수 있을 때

 [4단계 (선택)] 에이전틱 AI 시대의 ML 엔지니어링
   기간: 4주
   목표:
     - 모델을 FastAPI로 서빙 (API 엔드포인트 만들기)
     - 에이전트가 API를 호출하는 구조 체험
     - 모델 버전 관리 (MLflow)
     - 배포 후 성능 모니터링

   체크: '내가 만든 모델이 실제 서비스에서 작동한다'는 경험
"
```

→ **실무 적용**:
   2026년 채용공고의 70%가 "XGBoost + 배포 경험"을 찾고 있습니다.
   3단계까지는 필수, 4단계는 경쟁력입니다.

**[각 단계별 시간 투자 효율]**
```
1단계: 20시간 → 모델 돌리는 기초 (필수)
2단계: 30시간 → 성능 최적화 (필수)
3단계: 20시간 → 설명 가능성 (필수)
4단계: 40시간 → 배포 운영 (가산점)

총 110시간 = 약 6주 풀타임 or 4개월 파트타임
```

---

### → 슬라이드 5-13: 첫 주에 할 3가지

```
"바로 시작하고 싶으면, 이 3가지부터:

 [1] Kaggle 계정 만들고 첫 대회 연결
     www.kaggle.com
     → 'Getting Started' 대회 들어가기
     → 데이터 다운로드 + 첫 노트북 만들기

     왜: Kaggle은 ML 커뮤니티의 중심입니다.
        남의 코드를 읽고 배울 수 있어요.
        순위보다 학습이 목표입니다.

 [2] Titanic 데이터셋 해보기
     왜 Titanic인가?
     - 가장 유명한 입문 데이터셋
     - 결측, 이상치, 범주형 변수 섞여 있음
     - 1,000+ 개의 공개 노트북 → 참고 자료 풍부

     [Titanic으로 배울 수 있는 것]
     - EDA: 결측 / 성별-생존율 관계 / 클래스 별 생존율
     - Feature Engineering: 가족 수 → 혼자 탔는가 (파생변수)
     - 모델링: LogisticRegression → RandomForest → XGBoost
     - 평가: 정확도 → Confusion Matrix → AUC

 [3] 남의 코드 읽고 따라 하기
     왜: 처음엔 '똑같이' 해보는 게 가장 빠른 배움입니다.

     체크리스트:
     ☐ 코드를 읽었다
     ☐ 각 줄이 왜 필요한지 이해했다
     ☐ 변수명/함수명을 내 방식으로 바꿔봤다
     ☐ 주석을 내 말로 다시 써봤다
"
```

→ **실무 적용**:
   대부분의 ML 엔지니어는 Kaggle에서 시작했습니다.
   '순위'가 아니라 '배움'을 목표로 하세요.

**[예상 질문]** "Titanic은 너무 쉬운 거 아닌가요?"
→ "Titanic은 쉽지만, 완벽하게 할 줄 아느냐가 다릅니다.
   Titanic을 정확도 85%에서 90%로 올리는 과정이
   실무 모델 개선과 동일합니다.

   작은 데이터로 깊게 배우면,
   큰 데이터에서도 같은 원리를 적용할 수 있어요."

---

### → 슬라이드 5-14: 에이전틱 AI 시대 — ML 모르면 에이전트도 설계 못 한다

```
"2026년 AI 시장은 '에이전틱 AI'로 급변하고 있습니다.

 멀티에이전트 관련 문의가 작년 대비 1,445% 증가했어요. (Gartner)

 '에이전트'를 들으면, 자동으로 일을 처리하는 뭔가 고급 기술로 들립니다.
 하지만 내부를 열어보면:

 사용자 요청
   ↓
 [에이전트]
   ├─ 사용자 의도 파악
   ├─ 실행 계획 수립
   └─ 도구 호출 (여기가 중요!)
        ↓
      [ML 예측 모듈] ← 여기가 우리가 배운 것
         ├─ 고객 이탈 확률 예측
         ├─ 거래 이상거래 탐지
         └─ 상품 추천 점수 계산
        ↓
      [도구 실행]
         ├─ DB 업데이트
         ├─ 이메일 발송
         └─ 재고 조정
   ↓
 최종 결과

 이 구조에서 [ML 예측 모듈]이 없으면 에이전트는 동작하지 않아요.

 예시:
 '이탈 위험 고객에게 할인 쿠폰을 자동으로 보낸다'는 에이전트
 → 아무도 '이탈 위험 고객'을 정의할 수 없다면?
 → 에이전트는 모든 고객에게 쿠폰을 보내거나 (비용 폭증)
    아무도 안 보낼 거예요 (기회 상실)

 결론: ML 예측 모듈 = 에이전트의 눈과 귀"
```

→ **실무 적용**:
   2026년 신규 채용 공고:
   "MLOps 엔지니어 (에이전트 배포 경험)"
   "AI 아키텍트 (ML 시스템 설계)"
   → 모두 ML을 안다는 것이 전제조건입니다.

**[예상 질문]** "그럼 LLM으로 에이전트를 만들면 되지 않나요?"
→ "LLM은 자연어 이해가 뛰어나지만, '정확한 예측'은 약합니다.

   "내일 매출이 얼마일까?" → XGBoost + SHAP
   "왜 매출이 떨어졌을까?" → LLM (원인 해석)

   둘을 함께 쓸 때 에이전트가 완성돼요."

---

### → 슬라이드 5-15: 2026년 지금 시장 — 누가 ML을 시스템으로 운영하는가

```
"모델은 누구나 만들 수 있어요.

 'XGBoost 깔아서 10분 안에 정확도 90% 나왔어'

 이건 이제 놀랍지 않습니다.
 AutoML이 대부분을 자동으로 해주니까요.

 그럼 뭐가 차별점일까요?

 차별점 = '이 모델을 실제로 운영할 수 있는가'

 [운영이란]
 ├─ 배포: 모델을 API로 서빙
 ├─ 모니터링: 배포 후 성능이 떨어지지 않는가?
 ├─ 버전 관리: 이전 모델로 롤백 가능한가?
 ├─ 재학습: 새 데이터가 오면 자동으로 업데이트?
 └─ 설명: '이 예측이 왜 나왔는가' 실시간 답변

 이 모든 걸 할 수 있는 팀이 시장에서 이기고 있습니다.

 예시:
 A회사: XGBoost 정확도 93%
        근데 배포 후 2개월에 정확도 78%로 떨어짐
        원인 불명 (설명 불가)
        롤백 시스템 없어서 1주일 동안 망친 모델로 운영

 B회사: XGBoost 정확도 91%
        배포 후 성능 모니터링으로 71% 떨어짐 감지
        SHAP으로 이유 파악 (새로운 고객층에 과적합)
        자동 재학습 트리거
        → 3시간 후 다시 90% 회복

 누가 이기는가?
 → B회사입니다.

 모델 품질 93% vs 91% 차이는 무시할 수준이고,
 '운영 가능성'이 사업의 생명입니다."
```

→ **실무 적용**:
   스타트업 ML팀:
   "모델 만드는 것보다, 배포·모니터링·재학습이 더 어렵고 중요합니다"

   대기업 ML팀:
   "AutoML이 모델을 자동 생성하니까,
    이제 엔지니어들은 거의 운영에만 집중합니다"

**[3단계까지의 가치]**
```
[1단계] sklearn로 돌리기
   → 모델을 '만들 수 있다' (기초)

[2단계] XGBoost 튜닝
   → 모델을 '좋게 만들 수 있다' (성능)

[3단계] SHAP 해석
   → 모델을 '설명할 수 있다' (신뢰)

이 3단계 = 실무에서 '혼자 프로젝트를 끝낼 수 있는 수준'
```

---

## [클로징] 수미상관 + 마지막 메시지 (10분)

### → 슬라이드 5-16: 1차시 야구공으로 돌아오기

```
"1차시를 기억하나요?

 우리는 이 질문으로 시작했습니다:
 '왜 이 타자는 이 구종에만 약한가?'

 그때는 이 질문이 너무 멀게 느껴졌을 거예요.
 ML이 뭔지도 모르는데.

 지금은요?

 이 질문에 정확하게 답할 수 있어요.

 [풀이]
 1) 그 타자의 과거 타석 데이터 모아서 (EDA)
 2) XGBoost로 '구종별 타율 예측 모델' 만들어서 (1~4차시)
 3) SHAP으로 '이 타자-이 구종 조합이 낮은 이유'를 분석해요 (5차시)

 예를 들면:
 - 이 타자가 보는 구종의 회전수 패턴에 약함
   → SHAP: 회전수(RPM) → -30% 기여

 - 이 구종을 던지는 특정 투수와의 상성이 안 좋음
   → SHAP: 투수명(특정 투수) → -25% 기여

 - 이 타자가 약점 높이에 자리잡는 피치
   → SHAP: 높이(zone) → -20% 기여

 이제 이 타자 or 코치가 할 수 있는 게:
 '이 구종의 회전수를 보는 훈련하자'
 '이 투수의 릴리스 포인트를 집중 분석하자'

 → 데이터에서 나온 구체적 처방입니다."
```

**[6주 여정의 의미]**

```
1차시: '질문이 있었다'
       "왜 이 타자는..."

2~4차시: '질문에 답할 도구를 배웠다'
       "선형에서 XGBoost까지"

5차시: '답에 설득력을 더했다'
       "설명하고 공정하고 인과를 묻는다"

이제: '질문 → 분석 → 설명 → 실행'
     전체 사이클을 혼자 돌릴 수 있어요"
```

→ **실무 적용**:
   ML을 '도구'가 아니라 '문제 해결 방법론'으로 본 것.
   이게 입문자 vs 실무자의 가장 큰 차이입니다.

---

### → 슬라이드 5-17: 마지막 메시지 — "도구는 바뀌었고, 목적은 하나였다"

```
"1차시에 이런 얘기를 했어요.

 [1차시 인용]
 '10년 전엔 엑셀로 했고,
  그 다음엔 R 회귀 분석을 배웠고,
  그 다음엔 GPT로 답을 물어봤고,
  이제 ML과 에이전틱 AI까지 왔다.

  도구는 계속 바뀌었다.'

 그런데 정말 중요한 것:

 도구는 바뀌었지만, 목적은 단 하나였다:
 '좋은 질문을 던질 수 있는 사람이 되고 싶다'

 'ML을 배운다'는 건 알고리즘 이름을 외우는 게 아니라,
 '질문의 품질을 높이는 법'을 배우는 거였어요.

 [이 6주 동안 배운 실제 가치]

 X: XGBoost의 하이퍼파라미터 100개
 ✓: 데이터가 거진다는 걸 아는 감각

 X: SHAP Value 수식
 ✓: '왜 그런 예측을 했는가' 물을 수 있는 능력

 X: 함수명 암기
 ✓: '이 데이터는 신뢰할 수 있는가' 판단하는 눈

 X: 논문 읽기
 ✓: 남의 분석이 틀렸을 때 지적할 수 있는 근거

 이게 정말 중요한 거예요.

 왜냐면 도구는 또 바뀐다는 거 알죠?
 6개월 후엔 더 좋은 모델이 나올 거고,
 2년 후엔 지금 배운 게 낡을 수도 있어요.

 하지만 '질문하는 사람'은 영원하거든요.

 도구가 뭐든,
 '이게 정말 맞는가'를 물을 수 있으면,
 그 도구가 뭐든 잘 쓸 수 있어요.

 최종 메시지:
 '여기서 배운 모든 함수·수식은 구글링으로 다시 찾을 수 있어요.
  하지만 '질문하는 습관'은 여기서만 기를 수 있습니다.

  이제부터 여러분은
  도구가 바뀌어도 흔들리지 않는 사람이 됐어요.

  왜냐면 근본을 알고 있거든요."
```

**[마지막 당부]**

```
"도구를 배운 거예요.

 도구가 전부가 아니라는 걸 기억하세요.

 넘어질 때마다 '이 데이터는 왜 거진 거지?'
 'SHAP이 이상한데, 모델이 뭔가 잘못 학습한 건 아닐까?'
 '이 예측이 정말 공정한 건가?'

 이렇게 자꾸 의심하세요.

 이 의심이 좋은 ML 엔지니어를 만듭니다.

 도구는 또 바뀔 거니까,
 도구가 아니라 '물음표'를 가져가세요."
```

---

## [부록] 60분 타임라인

```
0~3분:   [연결] 4차시→5차시 브리지
         "세 가지가 더 있다"

3~23분:  [1부] 설명할 수 있어야 쓸 수 있다
         - 5-01: ML의 3가지 한계
         - 5-02: SHAP 개요
         - 5-03: 워터폴 차트 읽기
         - 5-04: Amazon 사례
         - 5-05: 시각화 원칙

23~38분: [2부] 데이터가 먼저다
         - 5-06: EDA 중요성
         - 5-07: 체크리스트
         - 5-08: 결측치 처리
         - 5-09: 이상치 의사결정
         - 5-10: Before/After
         - 5-11: 과제 소개

38~50분: [3부] 당신의 다음 단계
         - 5-12: 4단계 로드맵
         - 5-13: 첫 주 3가지
         - 5-14: 에이전트 시대
         - 5-15: 시장 현황

50~60분: [클로징] 수미상관
         - 5-16: 야구공 재현
         - 5-17: 마지막 메시지
```

---

## 작성 원칙 준수 체크

- [x] 수식 제거, 직관과 비유 중심
- [x] 베이스 없는 사람도 이해하는 비유 포함 (게임이론, 오답노트, 렌즈 등)
- [x] 각 개념 뒤에 "→ 실무에서는" 한 줄 포함
- [x] 클로징은 강하게 (자신감 전달)
- [x] 기존 session_flow.md의 좋은 멘트 재활용
- [x] 슬라이드 번호 명시 (5-01 형식)
- [x] [예상 질문] 태그 포함
- [x] [선택] 태그로 심화 내용 구분
